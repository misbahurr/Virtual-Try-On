{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9041235,"sourceType":"datasetVersion","datasetId":5450708},{"sourceId":9047628,"sourceType":"datasetVersion","datasetId":5454984},{"sourceId":9047634,"sourceType":"datasetVersion","datasetId":5454988}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install diffusers accelerate insightface onnxruntime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-30T08:49:04.740247Z","iopub.execute_input":"2024-07-30T08:49:04.740646Z","iopub.status.idle":"2024-07-30T08:49:50.686806Z","shell.execute_reply.started":"2024-07-30T08:49:04.740616Z","shell.execute_reply":"2024-07-30T08:49:50.685700Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.29.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting insightface\n  Downloading insightface-0.7.3.tar.gz (439 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting onnxruntime\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (6.11.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.13.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.23.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (9.5.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from insightface) (1.16.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from insightface) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from insightface) (3.7.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from insightface) (1.11.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from insightface) (1.2.2)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from insightface) (0.22.0)\nRequirement already satisfied: easydict in /opt/conda/lib/python3.10/site-packages (from insightface) (1.13)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from insightface) (3.0.8)\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (from insightface) (1.4.0)\nRequirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (from insightface) (3.9.0)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (1.13.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations->insightface) (0.0.4)\nRequirement already satisfied: opencv-python>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from albumentations->insightface) (4.10.0.84)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->insightface) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->insightface) (2023.12.9)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->insightface) (0.3)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.17.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->insightface) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->insightface) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->insightface) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->insightface) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->insightface) (2.9.0.post0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->insightface) (0.2.13)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.7.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->insightface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->insightface) (3.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.16.0)\nRequirement already satisfied: opencv-python-headless>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations->insightface) (4.10.0.84)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nDownloading diffusers-0.29.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: insightface\n  Building wheel for insightface (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for insightface: filename=insightface-0.7.3-cp310-cp310-linux_x86_64.whl size=884693 sha256=80e1ffeca80ee7f3b0b341153baf2790f5e4b208d16f32d1c868e96e0a742937\n  Stored in directory: /root/.cache/pip/wheels/e3/d0/80/e3773fb8b6d1cca87ea1d33d9b1f20a223a6493c896da249b5\nSuccessfully built insightface\nInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, diffusers, insightface\nSuccessfully installed coloredlogs-15.0.1 diffusers-0.29.2 humanfriendly-10.0 insightface-0.7.3 onnxruntime-1.18.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from diffusers import AutoPipelineForInpainting, AutoencoderKL\nfrom diffusers.utils import load_image\nimport torch\nimport IPython\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:35.255059Z","iopub.execute_input":"2024-07-31T15:21:35.255705Z","iopub.status.idle":"2024-07-31T15:21:35.260102Z","shell.execute_reply.started":"2024-07-31T15:21:35.255671Z","shell.execute_reply":"2024-07-31T15:21:35.259195Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:38.474744Z","iopub.execute_input":"2024-07-31T15:21:38.475087Z","iopub.status.idle":"2024-07-31T15:21:40.116875Z","shell.execute_reply.started":"2024-07-31T15:21:38.475062Z","shell.execute_reply":"2024-07-31T15:21:40.116098Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pipeline = AutoPipelineForInpainting.from_pretrained(\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n                                                     vae=vae,\n                                                     torch_dtype=torch.float16,\n                                                     variant=\"fp16\",\n                                                     use_safetensors=True\n                                                    ).to(\"cuda\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:42.937698Z","iopub.execute_input":"2024-07-31T15:21:42.938069Z","iopub.status.idle":"2024-07-31T15:21:57.227777Z","shell.execute_reply.started":"2024-07-31T15:21:42.938039Z","shell.execute_reply":"2024-07-31T15:21:57.226942Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a55aefa30d4f7e91376567a1c39965"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n","output_type":"stream"}]},{"cell_type":"code","source":"pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\", low_cpu_mem_usage=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:22:01.037642Z","iopub.execute_input":"2024-07-31T15:22:01.038276Z","iopub.status.idle":"2024-07-31T15:22:26.729686Z","shell.execute_reply.started":"2024-07-31T15:22:01.038243Z","shell.execute_reply":"2024-07-31T15:22:26.728809Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"image = load_image('/kaggle/input/peoples/000019_0.jpg').convert(\"RGB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:20:57.492204Z","iopub.execute_input":"2024-07-31T15:20:57.492826Z","iopub.status.idle":"2024-07-31T15:20:57.506507Z","shell.execute_reply.started":"2024-07-31T15:20:57.492790Z","shell.execute_reply":"2024-07-31T15:20:57.505736Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"IPython.display.Image(filename='/kaggle/input/peoples/000019_0.jpg') ","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:00.337309Z","iopub.execute_input":"2024-07-31T15:21:00.338180Z","iopub.status.idle":"2024-07-31T15:21:00.344916Z","shell.execute_reply.started":"2024-07-31T15:21:00.338147Z","shell.execute_reply":"2024-07-31T15:21:00.343943Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"image/jpeg":"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","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]},{"cell_type":"code","source":"print(image)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:04:29.904770Z","iopub.execute_input":"2024-07-26T17:04:29.905512Z","iopub.status.idle":"2024-07-26T17:04:29.910896Z","shell.execute_reply.started":"2024-07-26T17:04:29.905480Z","shell.execute_reply":"2024-07-26T17:04:29.910016Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"<PIL.Image.Image image mode=RGB size=192x256 at 0x7DFFBAF909D0>\n","output_type":"stream"}]},{"cell_type":"code","source":"ip_image = load_image('/kaggle/input/tshirts/000006_1.jpg').convert(\"RGB\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:04.591701Z","iopub.execute_input":"2024-07-31T15:21:04.592403Z","iopub.status.idle":"2024-07-31T15:21:04.611304Z","shell.execute_reply.started":"2024-07-31T15:21:04.592363Z","shell.execute_reply":"2024-07-31T15:21:04.610470Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"IPython.display.Image(filename='/kaggle/input/tshirts/000006_1.jpg') ","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:06.436852Z","iopub.execute_input":"2024-07-31T15:21:06.437689Z","iopub.status.idle":"2024-07-31T15:21:06.443877Z","shell.execute_reply.started":"2024-07-31T15:21:06.437653Z","shell.execute_reply":"2024-07-31T15:21:06.442920Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"image/jpeg":"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","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\nimport numpy as np\nimport cv2\nimport insightface\nfrom insightface.app import FaceAnalysis\nfrom PIL import Image, ImageDraw\n\n\n# Initialize face detection\napp = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n# Initialize segmentation pipeline\nsegmenter = pipeline(model=\"mattmdjaga/segformer_b2_clothes\")\n\n\ndef remove_face(img, mask):\n    # Convert image to numpy array\n    img_arr = np.asarray(img)\n    \n    # Run face detection\n    faces = app.get(img_arr)\n    \n    # Get the first face\n    faces = faces[0]['bbox']\n\n    # Width and height of face\n    w = faces[2] - faces[0]\n    h = faces[3] - faces[1]\n\n    # Make face locations bigger\n    faces[0] = faces[0] - (w*0.5) # x left\n    faces[2] = faces[2] + (w*0.5) # x right\n    faces[1] = faces[1] - (h*0.5) # y top\n    faces[3] = faces[3] + (h*0.2) # y bottom\n\n    # Convert to [(x_left, y_top), (x_right, y_bottom)]\n    face_locations = [(faces[0], faces[1]), (faces[2], faces[3])]\n\n    # Draw black rect onto mask\n    img1 = ImageDraw.Draw(mask)\n    img1.rectangle(face_locations, fill=0)\n\n    return mask\n\ndef segment_body(original_img, face=True):\n    # Make a copy\n    img = original_img.copy()\n    \n    # Segment image\n    segments = segmenter(img)\n\n    # Create list of masks\n    segment_include = [\"Hat\", \"Hair\", \"Sunglasses\", \"Upper-clothes\", \"Skirt\", \"Pants\", \"Dress\", \"Belt\", \"Left-shoe\", \"Right-shoe\", \"Face\", \"Left-leg\", \"Right-leg\", \"Left-arm\", \"Right-arm\", \"Bag\",\"Scarf\"]\n    mask_list = []\n    for s in segments:\n        if(s['label'] in segment_include):\n            mask_list.append(s['mask'])\n\n\n    # Paste all masks on top of eachother \n    final_mask = np.array(mask_list[0])\n    for mask in mask_list:\n        current_mask = np.array(mask)\n        final_mask = final_mask + current_mask\n            \n    # Convert final mask from np array to PIL image\n    final_mask = Image.fromarray(final_mask)\n\n    # Remove face\n    if(face==False):\n        final_mask = remove_face(img.convert('RGB'), final_mask)\n\n    # Apply mask to original image\n    img.putalpha(final_mask)\n\n    return img, final_mask\n\n\ndef segment_torso(original_img):\n    # Make a copy\n    img = original_img.copy()\n    \n    # Segment image\n    segments = segmenter(img)\n\n    # Create list of masks\n    segment_include = [\"Upper-clothes\", \"Dress\", \"Belt\", \"Face\", \"Left-arm\", \"Right-arm\"]\n    mask_list = []\n    for s in segments:\n        if(s['label'] in segment_include):\n            mask_list.append(s['mask'])\n\n\n    # Paste all masks on top of eachother \n    final_mask = np.array(mask_list[0])\n    for mask in mask_list:\n        current_mask = np.array(mask)\n        final_mask = final_mask + current_mask\n            \n    # Convert final mask from np array to PIL image\n    final_mask = Image.fromarray(final_mask)\n\n    # Remove face\n    final_mask = remove_face(img.convert('RGB'), final_mask)\n\n    # Apply mask to original image\n    img.putalpha(final_mask)\n\n    return img, final_mask\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:09.038297Z","iopub.execute_input":"2024-07-31T15:21:09.038818Z","iopub.status.idle":"2024-07-31T15:21:19.179934Z","shell.execute_reply.started":"2024-07-31T15:21:09.038626Z","shell.execute_reply":"2024-07-31T15:21:19.179026Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\nfind model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\nfind model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\nfind model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\nfind model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\nApplied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\nfind model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\nset det-size: (640, 640)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5887880e480f452eb7b3ce8ec35cdb28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb465aeca622473aad70af6688b71d82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f522faafda4c4035bdfc7926bf005d04"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"seg_image, mask_image = segment_body(image, face=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:21:23.452191Z","iopub.execute_input":"2024-07-31T15:21:23.452598Z","iopub.status.idle":"2024-07-31T15:21:25.814697Z","shell.execute_reply.started":"2024-07-31T15:21:23.452564Z","shell.execute_reply":"2024-07-31T15:21:25.813634Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n","output_type":"stream"}]},{"cell_type":"code","source":"pipeline.set_ip_adapter_scale(1.0)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:23:42.562373Z","iopub.execute_input":"2024-07-31T15:23:42.562788Z","iopub.status.idle":"2024-07-31T15:23:42.603652Z","shell.execute_reply.started":"2024-07-31T15:23:42.562757Z","shell.execute_reply":"2024-07-31T15:23:42.602850Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"final_image = pipeline(\n    prompt=\"photorealistic, perfect body, beautiful skin, realistic skin, natural skin\",\n    negative_prompt=\"ugly, bad quality, bad anatomy, deformed feet, deformed face, deformed clothing, deformed skin, leggings, tights, stockings, bad skin, deformed body, deformed hands\",\n    image=image,\n    mask_image=mask_image,\n    ip_adapter_image=ip_image,\n    strength=0.99,  # Reduced strength to 0.75 to prevent over-adjustment\n    guidance_scale=9.5,  # Increased guidance scale for better adherence to the prompt\n    num_inference_steps=200,  # Increased the number of inference steps for finer details\n).images[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:24:24.405515Z","iopub.execute_input":"2024-07-31T15:24:24.406529Z","iopub.status.idle":"2024-07-31T15:30:38.257546Z","shell.execute_reply.started":"2024-07-31T15:24:24.406484Z","shell.execute_reply":"2024-07-31T15:30:38.256672Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e164c1db46b4a478896990dc1d1a72b"}},"metadata":{}}]},{"cell_type":"code","source":"final_image.save('/kaggle/working/final_image1.jpg')\n\n# Display the final image\n","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:37:38.808839Z","iopub.execute_input":"2024-07-31T15:37:38.809729Z","iopub.status.idle":"2024-07-31T15:37:38.819001Z","shell.execute_reply.started":"2024-07-31T15:37:38.809692Z","shell.execute_reply":"2024-07-31T15:37:38.818216Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model for CPU usage\npipeline.to(\"cpu\")\npipeline.save_pretrained(\"/kaggle/working/saved_model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:37:45.460267Z","iopub.execute_input":"2024-07-31T15:37:45.461138Z","iopub.status.idle":"2024-07-31T15:38:33.251298Z","shell.execute_reply.started":"2024-07-31T15:37:45.461103Z","shell.execute_reply":"2024-07-31T15:38:33.250238Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\nPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\nPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\nPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\nPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:53:13.695533Z","iopub.execute_input":"2024-07-31T15:53:13.695940Z","iopub.status.idle":"2024-07-31T16:02:37.603525Z","shell.execute_reply.started":"2024-07-31T15:53:13.695904Z","shell.execute_reply":"2024-07-31T16:02:37.602293Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/saved_model/ (stored 0%)\n  adding: kaggle/working/saved_model/feature_extractor/ (stored 0%)\n  adding: kaggle/working/saved_model/feature_extractor/preprocessor_config.json (deflated 48%)\n  adding: kaggle/working/saved_model/vae/ (stored 0%)\n  adding: kaggle/working/saved_model/vae/diffusion_pytorch_model.safetensors (deflated 8%)\n  adding: kaggle/working/saved_model/vae/config.json (deflated 58%)\n  adding: kaggle/working/saved_model/unet/ (stored 0%)\n  adding: kaggle/working/saved_model/unet/diffusion_pytorch_model.safetensors (deflated 8%)\n  adding: kaggle/working/saved_model/unet/config.json (deflated 61%)\n  adding: kaggle/working/saved_model/tokenizer/ (stored 0%)\n  adding: kaggle/working/saved_model/tokenizer/vocab.json (deflated 71%)\n  adding: kaggle/working/saved_model/tokenizer/special_tokens_map.json (deflated 73%)\n  adding: kaggle/working/saved_model/tokenizer/tokenizer_config.json (deflated 63%)\n  adding: kaggle/working/saved_model/tokenizer/merges.txt (deflated 60%)\n  adding: kaggle/working/saved_model/text_encoder/ (stored 0%)\n  adding: kaggle/working/saved_model/text_encoder/model.safetensors (deflated 8%)\n  adding: kaggle/working/saved_model/text_encoder/config.json (deflated 44%)\n  adding: kaggle/working/saved_model/image_encoder/ (stored 0%)\n  adding: kaggle/working/saved_model/image_encoder/model.safetensors (deflated 15%)\n  adding: kaggle/working/saved_model/image_encoder/config.json (deflated 45%)\n  adding: kaggle/working/saved_model/tokenizer_2/ (stored 0%)\n  adding: kaggle/working/saved_model/tokenizer_2/vocab.json (deflated 71%)\n  adding: kaggle/working/saved_model/tokenizer_2/special_tokens_map.json (deflated 72%)\n  adding: kaggle/working/saved_model/tokenizer_2/tokenizer_config.json (deflated 68%)\n  adding: kaggle/working/saved_model/tokenizer_2/merges.txt (deflated 60%)\n  adding: kaggle/working/saved_model/model_index.json (deflated 60%)\n  adding: kaggle/working/saved_model/scheduler/ (stored 0%)\n  adding: kaggle/working/saved_model/scheduler/scheduler_config.json (deflated 50%)\n  adding: kaggle/working/saved_model/text_encoder_2/ (stored 0%)\n  adding: kaggle/working/saved_model/text_encoder_2/model.safetensors\nzip I/O error: No space left on device\nzip error: Output file write failure (write error on zip file)\n","output_type":"stream"}]},{"cell_type":"code","source":"pipeline = AutoPipelineForInpainting.from_pretrained(\"/kaggle/working/saved_model\", torch_dtype=torch.float32).to(\"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the images\nimage = load_image('/kaggle/input/peoples/000006_0.jpg').convert(\"RGB\")\ndisplay(IPython.display.Image(filename='/kaggle/input/peoples/000006_0.jpg'))\nip_image = load_image('/kaggle/input/tshirts/000004_1.jpg').convert(\"RGB\")\ndisplay(IPython.display.Image(filename='/kaggle/input/tshirts/000004_1.jpg'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n\n# Initialize face detection on CPU\napp = FaceAnalysis(providers=['CPUExecutionProvider'])\napp.prepare(ctx_id=-1, det_size=(640, 640))\n\n# Initialize segmentation pipeline\nsegmenter = pipeline(model=\"mattmdjaga/segformer_b2_clothes\")\n\ndef remove_face(img, mask):\n    img_arr = np.asarray(img)\n    faces = app.get(img_arr)\n    faces = faces[0]['bbox']\n    w = faces[2] - faces[0]\n    h = faces[3] - faces[1]\n    faces[0] -= w * 0.5\n    faces[2] += w * 0.5\n    faces[1] -= h * 0.5\n    faces[3] += h * 0.2\n    face_locations = [(faces[0], faces[1]), (faces[2], faces[3])]\n    img1 = ImageDraw.Draw(mask)\n    img1.rectangle(face_locations, fill=0)\n    return mask\n\ndef segment_body(original_img, face=True):\n    img = original_img.copy()\n    segments = segmenter(img)\n    segment_include = [\"Hat\", \"Hair\", \"Sunglasses\", \"Upper-clothes\", \"Skirt\", \"Pants\", \"Dress\", \"Belt\", \"Left-shoe\", \"Right-shoe\", \"Face\", \"Left-leg\", \"Right-leg\", \"Left-arm\", \"Right-arm\", \"Bag\",\"Scarf\"]\n    mask_list = [s['mask'] for s in segments if s['label'] in segment_include]\n    final_mask = np.array(mask_list[0])\n    for mask in mask_list:\n        final_mask += np.array(mask)\n    final_mask = Image.fromarray(final_mask)\n    if not face:\n        final_mask = remove_face(img.convert('RGB'), final_mask)\n    img.putalpha(final_mask)\n    return img, final_mask\n\n# Segment body and process the image\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seg_image, mask_image = segment_body(image, face=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline.set_ip_adapter_scale(1.0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_image = pipeline(\n    prompt=\"photorealistic, perfect body, beautiful skin, realistic skin, natural skin\",\n    negative_prompt=\"ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings\",\n    image=image,\n    mask_image=mask_image,\n    ip_adapter_image=ip_image,\n    strength=0.75,\n    guidance_scale=9.5,\n    num_inference_steps=250,\n).images[0]\n\n# Save and display the final image\nfinal_image.save('/kaggle/working/final_image_cpu.jpg')\ndisplay(final_image)","metadata":{},"execution_count":null,"outputs":[]}]}